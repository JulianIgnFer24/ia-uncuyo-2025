# Trabajo Pr√°ctico 7A: Introducci√≥n a ML

## 1. Flexibilidad de los M√©todos de Aprendizaje de M√°quinas

### a) $n$ extremadamente grande, $p$ peque√±o

Respuesta: m√©todo flexible 


Justificaci√≥n:
En este escenario ($n$ grande, $p$ peque√±o), se espera que un **m√©todo flexible se comporte mejor**. La raz√≥n principal es que la gran cantidad de observaciones ($n$) mitiga el principal riesgo de los modelos flexibles: la alta varianza o sobreajuste. Un modelo flexible tiene un sesgo bajo, lo que le permite capturar patrones complejos o no lineales en $f$. Si bien esto normalmente podr√≠a llevarlo a "memorizar" el ruido en muestras peque√±as, un $n$ grande le permite distinguir el ruido de la se√±al verdadera. Adem√°s, un $p$ peque√±o evita la maldici√≥n de la dimensionalidad, asegurando que los datos cubran densamente el espacio de predictores, lo que hace que la estimaci√≥n de $f$ por parte del modelo flexible sea m√°s estable y precisa.

### b) $p$ extremadamente grande, $n$ peque√±o

Respuesta: no flexibe 

Justificaci√≥n: 
En este escenario, es mejor utilizar un m√©todo no flexible. El peque√±o $n$ eleva el riesgo de sobreajuste (overfitting), mientras que el gran $p$ causa la "maldici√≥n de la dimensionalidad", dispersando los pocos datos en un espacio de caracter√≠sticas enorme. Un modelo flexible (de baja sesgo) tendr√≠a una varianza alt√≠sima al intentar ajustarse al ruido en este espacio disperso. Por el contrario, un m√©todo inflexible (de alto sesgo) impone una estructura simple que controla la varianza, lo cual es esencial para prevenir el sobreajuste en esta situaci√≥n.

### c) La relaci√≥n entre predictores y variable dependiente es altamente no lineal

Respuesta: flexibe

Justificaci√≥n: Se necesita un m√©todo flexible, sino, es probable que no se pueda capturar la "forma" de la funci√≥n f. 


### d) La varianza de los t√©rminos de error, $\sigma^2 = \text{Var}(\epsilon)$, es extremadamente alta

Respuesta: no flexible

Justificaci√≥n:
Cuando $\sigma^2$ (la varianza del error) es extremadamente alta, los datos est√°n muy "ruidosos". Esto significa que las observaciones $Y$ est√°n muy dispersas y lejos de la verdadera funci√≥n $f(X)$.

---

## 2. Clasificaci√≥n vs. Regresi√≥n e Inferencia vs. Predicci√≥n

### a) Salario de Directores Ejecutivos

* Inferencia: qu√© factores afectan al salario
* Regresi√≥n: qu√© salio tienen los directores ejecutivos 
* $n$:500
* $p$:3


### b) √âxito o Fracaso de Nuevo Producto

* Prediccion: si es exito o fracaso 
* Clasificacion: Si es exito o fracazo 
* $n$: 20
* $p$: 13

### c) Predicci√≥n del Tipo de Cambio USD/Euro

* Regrecion
* Prediccion
* $n$: 52
* $p$: 3

---

## 3. Ventajas y Desventajas de la Flexibilidad

### Ventajas de un enfoque flexible
- Puede **capturar relaciones no lineales** o complejas entre los predictores y la variable de salida.   
- Es ideal cuando se dispone de **muchos datos** y la verdadera relaci√≥n subyacente es complicada.


### Desventajas de un enfoque flexible
- Tiende a tener **mayor varianza**, lo que significa que puede **sobreajustar (overfitting)** los datos de entrenamiento.  
- Requiere **m√°s datos y poder computacional** para entrenarse correctamente.  
- Es m√°s dif√≠cil de **interpretar**, ya que las relaciones aprendidas pueden ser muy complejas.

### Ventajas de un enfoque inflexible
- Tiende a tener **menor varianza**, por lo que es **m√°s estable y menos propenso al sobreajuste (overfitting)**.  
- Funciona bien cuando el **n√∫mero de observaciones es peque√±o** o cuando los datos contienen **mucho ruido**.  
- Es **m√°s f√°cil de interpretar**, ya que las relaciones entre las variables suelen ser simples y directas.  

---

### Desventajas de un enfoque inflexible  
- Su rendimiento disminuye cuando la **verdadera relaci√≥n en los datos es altamente no lineal o complicada**.  
- Puede producir **predicciones menos precisas** si el modelo es demasiado r√≠gido para la naturaleza del problema.


### Cu√°ndo preferir un enfoque m√°s flexible
- Cuando se dispone de **una gran cantidad de datos (n grande)**.  
- Cuando la relaci√≥n entre los predictores y la variable objetivo es **altamente no lineal o compleja**.  
- Cuando el objetivo principal es **maximizar la precisi√≥n de la predicci√≥n**, m√°s que la interpretabilidad.


### Cu√°ndo preferir un enfoque menos flexible
- Cuando el conjunto de datos es **peque√±o o ruidoso** (alta varianza en los errores).  
- Cuando se sospecha que la relaci√≥n entre las variables es **simple o aproximadamente lineal**.  
- Cuando se busca **interpretabilidad** y **robustez** m√°s que exactitud extrema.

---

## 4. Enfoque Param√©trico vs. No Param√©trico

Los **enfoques param√©tricos** y **no param√©tricos** difieren principalmente en la forma en que modelan la relaci√≥n entre los predictores y la variable objetivo.

- Un **modelo param√©trico** asume una **forma funcional espec√≠fica** para la relaci√≥n entre las variables (por ejemplo, lineal, cuadr√°tica, log√≠stica, etc.).  
  Una vez elegida esa forma, el aprendizaje consiste en **estimar los par√°metros** del modelo a partir de los datos.

- Un **modelo no param√©trico**, en cambio, **no impone una forma funcional fija**.  
  En lugar de eso, **deja que los datos determinen la estructura del modelo**, permitiendo una mayor flexibilidad para capturar relaciones complejas o no lineales.

---

### üîπ Diferencias principales

| Aspecto | Enfoque Param√©trico | Enfoque No Param√©trico |
|----------|--------------------|------------------------|
| **Suposici√≥n de forma del modelo** | Se asume una forma espec√≠fica (ej. lineal) | No se asume una forma fija |
| **N√∫mero de par√°metros** | Fijo, independiente de *n* | Crece con *n* |
| **Flexibilidad** | Menos flexible | M√°s flexible |
| **Requisitos de datos** | Funciona bien con pocos datos | Requiere muchos datos |
| **Complejidad computacional** | Baja | Alta |
| **Interpretabilidad** | Alta | Baja |
| **Ejemplos t√≠picos** | Regresi√≥n lineal, regresi√≥n log√≠stica | KNN, √°rboles de decisi√≥n, redes neuronales |

---

### Ventajas del enfoque param√©trico
- **Simplicidad:** f√°cil de interpretar y de implementar.  
- **Menor riesgo de sobreajuste:** al estar restringido por una forma fija.  
- **Eficiencia computacional:** requiere menos recursos para entrenar.  
- **Buen rendimiento con pocos datos**, si la forma asumida del modelo es razonable.

---

### Desventajas del enfoque param√©trico
- **Alta dependencia de las suposiciones:** si la forma funcional elegida no refleja bien la realidad, el modelo tiene **alto sesgo (bias)**.  
- **Menor flexibilidad:** no captura bien relaciones complejas o no lineales.  

---

### Ventajas del enfoque no param√©trico
- **Alta flexibilidad:** puede modelar relaciones no lineales o complejas sin suponer una forma espec√≠fica.  
- **Mejor ajuste cuando se dispone de muchos datos.**

---

### Desventajas del enfoque no param√©trico
- **Mayor varianza:** propenso al sobreajuste si los datos son pocos o ruidosos.  
- **Menor interpretabilidad.**  
- **Mayor costo computacional** y necesidad de m√°s datos para generalizar bien.
---

## 5. K Vecinos M√°s Cercanos (KNN) para Clasificaci√≥n

Punto de prueba: $X_1 = 0, X_2 = 0, X_3 = 0$.

### a) Distancia Euclidiana

| Obs. | $X_1$ | $X_2$ | $X_3$ | Distancia Euclidiana ($D_i$) |
| :---: | :---: | :---: | :---: | :---: |
| 1 | 0 | 3 | 0 | 3.000 |
| 2 | 2 | 0 | 0 | 2.000 |
| 3 | 0 | 1 | 3 | 3.162 |
| 4 | 0 | 1 | 2 | 2.236 |
| 5 | -1 | 0 | 1 | 1.414 |
| 6 | 1 | 1 | 1 | 1.732 |

### b) Predicci√≥n con $K = 1$

* Predicci√≥n: Verde
* Justificaci√≥n: porque el vecino mas cercado que es la observacion 5 y su color es verde

### c) Predicci√≥n con $K = 3$

* Predicci√≥n: Rojo 
* Justificaci√≥n: porque tengo 2 observaciones rojas y una verde 

### d) Valor de $K$ si el l√≠mite de decisi√≥n de Bayes es altamente no lineal

* Valor esperado de $K$: Si el **l√≠mite de decisi√≥n de Bayes** es altamente **no lineal**, se espera que el **mejor valor de K sea peque√±o**.

* Raz√≥n: Cuando esa frontera es **compleja o muy curva**, un modelo **flexible** es necesario para poder aproximarla adecuadamente. En el caso del algoritmo **K-Nearest Neighbors (KNN)**: Un **K peque√±o** permite que el modelo sea **muy sensible a los patrones locales**, adapt√°ndose mejor a una frontera no lineal.  